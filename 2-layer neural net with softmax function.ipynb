{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 2-layer Neural Network with Softmax Function\n",
    "+ Simple 2-layer neural network with softmax function for multi-class classification.\n",
    "+ Classify red ball, green ball or blue ball for a given 2D input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "eta = 0.7  # learning rate\n",
    "epoch = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "+  For softmax function, x - max(x)  is used to avoid overflow of exponent function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    # softmax   \n",
    "    e = np.exp(x - np.max(x)) # x-m is used to avoid overflow of exponent function\n",
    "    return e / np.sum(e, axis=0) # add in column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer Neural Network Model with Softmax Function\n",
    "+ Softmax function is used at outputs of layer 2.\n",
    "+ Total error is also obtained by summing up the individual error and averaging them. \n",
    "+ For more details on the definition of delta function, refer to the class note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnetwork:\n",
    "    # neural network model\n",
    "    \n",
    "    def __init__(self, x, w1, w2, y):\n",
    "        self.inputs   = x.T\n",
    "        self.weights1 = w1\n",
    "        self.weights2 = w2\n",
    "        self.b1 = np.zeros((4,1)) # bias at hidden layer \n",
    "        self.b2 = np.zeros((3,1)) # bias at output layer \n",
    "        self.target   = y.T       # target\n",
    "        self.output   = np.zeros(self.target.shape)\n",
    "\n",
    "    def forwardprop(self):\n",
    "        # forward processing of inputs and weights using sigmoid activation function\n",
    "        self.hiddenout = sigmoid(np.dot(self.weights1, self.inputs) + self.b1)\n",
    "        self.output = softmax(np.dot(self.weights2, self.hiddenout) + self.b2)\n",
    "\n",
    "    def backprop(self):\n",
    "        # backward processing of appling the chain rule to find derivative of the loss function with respect to weights\n",
    "        delta2 = (self.output - self.target)  \n",
    "        delta1 = np.dot(self.weights2.T, delta2) * sigmoid_deriv(self.hiddenout)\n",
    "        dw2 = np.dot(delta2, self.hiddenout.T)\n",
    "        dw1 = np.dot(delta1, self.inputs.T)\n",
    "\n",
    "        # update the weights with the derivative of the loss function\n",
    "        self.weights1 -= eta * dw1\n",
    "        self.weights2 -= eta * dw2\n",
    "\n",
    "        # update biases with the derivative of the loss function\n",
    "        self.b2 -= eta * np.sum(delta2, axis = 1, keepdims=True) # add in row axis, keeping column dimension\n",
    "        self.b1 -= eta * np.sum(delta1, axis = 1, keepdims=True) # add in row axis, keeping column dimension\n",
    "\n",
    "    def predict(self, x):\n",
    "        # predict the output for a given input x\n",
    "        self.hiddenout = sigmoid(np.dot(self.weights1, x))\n",
    "        self.output = sigmoid(np.dot(self.weights2, self.hiddenout))\n",
    "        return (self.output)\n",
    "        \n",
    "     # calculate error\n",
    "    def calculate_error(self):\n",
    "        error = np.sum(-self.target * np.log(self.output)) # cross entropy loss function for multi-class classification\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "+ For simplicity, it is assumed that the color of ball can be identified with two features, i.e., x_axis and y_axis values.\n",
    "+ (x, y) values of red, green and blue balls are centered around (0, -3), (3, 3) and (-3, 3), respectively.\n",
    "+ Target values for red, green and blue balls are [1, 0, 0], [0, 1, 0] and [0, 0, 1], respectively.\n",
    "+ Formats of input data and targetvalue are shown below, which are basically lists of row vectors.\n",
    "+ In program, these data are converted to column vectors.\n",
    "+\n",
    "+  input data = \n",
    "+ [[0.1,0.3],  # input data item1\n",
    "+ [0.5,0.8],  # input data item2\n",
    "+ [0.7,0.6],  # input data item3\n",
    "+ [0.9,0.2]] # input data item4\n",
    "+   \n",
    "+  target data = \n",
    "+ [[0,1,0],  # target1, one-hot vector representation\n",
    "+  [1,0,0],  # target2, one-hot vector representation\n",
    "+  [0,0,1],  # target3, one-hot vector representation\n",
    "+  [1,0,0]]  # target4, one-hot vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # get training data set\n",
    "    samples = 10\n",
    "    attributes = 2\n",
    "    classes = 3\n",
    "    \n",
    "    np.random.seed(45)\n",
    "    data1_set = np.random.randn(samples, attributes) + np.array([0, -3])  # red ball\n",
    "    data2_set = np.random.randn(samples, attributes) + np.array([3,  3])  # green ball\n",
    "    data3_set = np.random.randn(samples, attributes) + np.array([-3, 3])  # blue ball\n",
    "   \n",
    "    feature_set = np.vstack([data1_set, data2_set, data3_set])   \n",
    "    labels = np.array([0]*samples + [1]*samples + [2]*samples)\n",
    "\n",
    "    # Initializing colors and building a colormap\n",
    "    cmap = mpl.colors.ListedColormap(['red', 'green', 'blue'])\n",
    "\n",
    "    # display data training data set\n",
    "    # plt.figure(figsize=(10,7)) \n",
    "    plt.scatter(feature_set[:,0], feature_set[:,1], c=labels, cmap =cmap, s= 200, alpha=0.5)  \n",
    "    plt.show()\n",
    "\n",
    "    one_hot_labels = np.zeros((samples*classes, classes))\n",
    "    \n",
    "    for i in range(samples*classes):  \n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "\n",
    "    inputdata = feature_set\n",
    "    targetvalue = one_hot_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent Optimization\n",
    "+ All the input data are processed in batch at both forward and backward propagations.\n",
    "+ In comparison with SGD, batch GD optimizes more smoothly since the weights update are performed in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    w2 = np.random.rand(3, 4)     # number of output labels is 3\n",
    "    w1 = np.random.rand(4, inputdata.shape[1])  # number of nodes at a hidden layer is 4\n",
    "\n",
    "    nn = neuralnetwork(inputdata, w1, w2, targetvalue)\n",
    "\n",
    "    training_loss = []  \n",
    "  \n",
    "    # training \n",
    "    for i in range(epoch):    \n",
    "        nn.forwardprop()\n",
    "        nn.backprop()\n",
    "        if (i % 500) == 0:\n",
    "            print(\"Error: \", nn.calculate_error())\n",
    "            training_loss.append(nn.calculate_error()) # store training loss \n",
    " \n",
    "    print(\"target\")   \n",
    "    print(targetvalue)\n",
    "    print(\"output after training\")   \n",
    "    print(nn.output.T)\n",
    "\n",
    "    # predicting and testing the output for a given input data\n",
    "    # For a given input data [1, -1], it is predicated as red ball since the output is [0, 1, 0], i.e. the red ball.\n",
    "    x_prediction = np.array([[1.0, -1.0]])\n",
    "    predicted_output = nn.predict(x_prediction.T)\n",
    "    print(\"Predicted data based on trained weights: \")\n",
    "    print(\"Input: \", x_prediction)\n",
    "    print(\"Output: \", predicted_output.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: Error display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    print(training_loss)\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_loss) + 1)\n",
    "#    plt.figure(figsize=(15,5))\n",
    "    plt.plot(epoch_count, training_loss, 'b-')\n",
    "    plt.legend(['Training Loss'])\n",
    "    plt.xlabel('Epoch(500)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
