{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 2-layer Neural Net with Vectorization\n",
    "+ In forward and backward propagations, data are processed in batch, using maxtrix multiplication.\n",
    "+ The MSE loss function is adopted, and for simplicity, bias is not considered.\n",
    "+ It is shown that XOR operation can be achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "eta = 0.9  # learning rate\n",
    "epoch = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer Neural Network Model\n",
    "+ Sigmoid activation functions (a.k.a logistic function) are used at outputs of both layer 1 and layer 2.\n",
    "+ Total error is also obtained by summing up the individual error and averaging them. \n",
    "+ The weight is updated by first summing up all the errors and then deriving the change of the weight derivative\n",
    "+ which is associated witht the total error. \n",
    "+ The calulation of these operation is perfomed by dot product of delta and output of each layer.\n",
    "+ For more details on the definition of delta function, refer to the class note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "class neuralnetwork:\n",
    "    # neural network model\n",
    "    \n",
    "    def __init__(self, x, w1, w2, y):\n",
    "        self.inputs   = x.T\n",
    "#        print(self.inputs)\n",
    "        self.weights1 = w1\n",
    "        self.weights2 = w2\n",
    "        self.target   = y\n",
    "        self.output   = np.zeros(self.target.shape)\n",
    "\n",
    "    def forwardprop(self):\n",
    "        # forward processing of inputs and weights using sigmoid activation function\n",
    "        self.layer1 = sigmoid(np.dot(self.weights1, self.inputs))\n",
    "        self.output = sigmoid(np.dot(self.weights2, self.layer1))\n",
    "\n",
    "    def backprop(self):\n",
    "        # backward processing of appling the chain rule to find derivative of the loss function with respect to weights\n",
    "        delta2 = (self.output - self.target) * sigmoid_deriv(self.output)\n",
    "        delta1 = np.dot(self.weights2.T, delta2) * sigmoid_deriv(self.layer1)\n",
    "        dw2 = np.dot(delta2, self.layer1.T)\n",
    "        dw1 = np.dot(delta1, self.inputs.T)\n",
    "\n",
    "        # update the weights with the derivative of the loss function\n",
    "        self.weights1 -= eta * dw1 / batchsize\n",
    "        self.weights2 -= eta * dw2 / batchsize\n",
    "\n",
    "    def predict(self, x):\n",
    "        # predict the output for a given input x\n",
    "        self.layer1 = sigmoid(np.dot(self.weights1, x))\n",
    "        self.output = sigmoid(np.dot(self.weights2, self.layer1))\n",
    "        return (self.output)\n",
    "        \n",
    "    def calculaterror(self):\n",
    "        # calculate error\n",
    "        error = self.target - self.output\n",
    "#        print(\"Output: \", self.output)\n",
    "        return str(np.mean(np.abs(error)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent Optimization\n",
    "+ All the input data are processed in batch at both forward and backward propagations.\n",
    "+ In comparison with SGD, batch GD optimizes more smoothly since the weights update are performed in batch, \n",
    "+ differently from that of SGD. In SGD, the weight update is acheived individually for each input data, using\n",
    "+ the weigth of the other input data as initial weight at each step of optimization. \n",
    "+ This results in non-smooth path of SGD optimizaation, comparing with that of BGD (batch gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    " \n",
    "    inputdata = np.array([[0,0],\n",
    "                          [0,1],\n",
    "                          [1,0],\n",
    "                          [1,1]])\n",
    "    \n",
    "    batchsize = inputdata.shape[0]\n",
    "    w2 = np.random.rand(1, 4)\n",
    "    w1 = np.random.rand(4, inputdata.shape[1]) \n",
    "\n",
    "    targetvalue = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn = neuralnetwork(inputdata, w1, w2, targetvalue)\n",
    "  \n",
    "    # training \n",
    "    for i in range(epoch):    \n",
    "        nn.forwardprop()\n",
    "        nn.backprop()\n",
    "        if (i % 1000) == 0:\n",
    "            print(\"Error: \", nn.calculaterror())\n",
    "        \n",
    "    print(\"output after training\")   \n",
    "    print(nn.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Prediction\n",
    "+ After training, you can verify that the required target is generated for a given input data.\n",
    "+ It can be verified that the XOR operation is achieved.\n",
    "+ Although the above neural net has one hidden layer, it may process much more complex input data. \n",
    "+ Theoretically, according to the universal approximation theorem, 2-layer neural net can approximate arbitary \n",
    "+ continuous function under mild assumptions on the activation function.\n",
    "+ For more details on universal approximation theorem, refer to \n",
    "+ https://en.wikipedia.org/wiki/Universal_approximation_theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # predicting and testing the output for a given input data\n",
    "    x_prediction = np.array([[1, 1]])\n",
    "    predicted_output = nn.predict(x_prediction.T)\n",
    "    print(\"Predicted data based on trained weights: \")\n",
    "    print(\"Input: \", x_prediction)\n",
    "    print(\"Output: \", predicted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
