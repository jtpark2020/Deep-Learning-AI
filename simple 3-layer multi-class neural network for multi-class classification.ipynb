{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 3-layer multi-class neural network for multi-class classification\n",
    "+ Network architecture: number of neurons at layer 1, layer 2, output = (4, 5, 3)\n",
    "+ Activation functions employed at layer 1, layer 2 and output are ReLu, sigmoid, and softmax, respectively.\n",
    "+ In addition to adding another layer, the ReLu activation function is adopted at layer 1 instead of sigmoid funtion.\n",
    "+ ReLu tends to prevent the weights not to be drastically reduced as the number of layers grows. \n",
    "+ By expermentation, it can be shown that the optimization depends on the learning rate.\n",
    "+ For example, the learning rate of 0.7 does not lead to the optimization. This leads to the development of more efficient\n",
    "+ optimization technique such as moment, ADAM, RMSprop, etc. These techniques try to adjust the learning rate in accordance with\n",
    "+ specific property of features of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "eta = 0.9 # learning rate\n",
    "epoch = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu Activation Function\n",
    "+ ReLu activation function and its derivative are defined.\n",
    "+ ReLU stands for rectified linear unit, and is a type of activation function. Mathematically, it is defined as \n",
    "+ y = max(0, + x). Visually, it looks like the following: ReLU is the most commonly used activation function in neural \n",
    "+ networks, especially in CNNs. The rectified linear activation function overcomes the vanishing gradient problem, \n",
    "+ allowing models to learn faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    # softmax   \n",
    "    e = np.exp(x - np.max(x))    # x-m is used to avoid overflow of exponent function\n",
    "    return e / np.sum(e, axis=0) # add in column\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-layer Neural Network Model with Softmax Function\n",
    "+ Softmax function is used at outputs of layer 3.\n",
    "+ ReLu is used for layer 1 activation function, sigmoid for layer 2, and finally softmax for layer 3, i.e., output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnetwork:\n",
    "    # neural network model\n",
    "    \n",
    "    def __init__(self, x, w1, w2, w3, y):\n",
    "        self.inputs   = x.T\n",
    "        self.target  = y.T        # target\n",
    "        self.weights1 = w1        # weights at layer 1\n",
    "        self.weights2 = w2        # weights at layer 2\n",
    "        self.weights3 = w3        # weights at output layer\n",
    "        self.b1 = np.zeros((4,1)) # bias at layer 1 \n",
    "        self.b2 = np.zeros((5,1)) # bias at layer 2 \n",
    "        self.b3 = np.zeros((3,1)) # bias at output layer   \n",
    "        self.layer1  = np.zeros((4, self.inputs.shape[1])) # output at layer 1\n",
    "        self.layer2  = np.zeros((5, self.inputs.shape[1])) # output at layer 2\n",
    "        self.output   = np.zeros(self.target.shape) # output of output layer\n",
    "    \n",
    "    def forwardprop(self):\n",
    "        # forward processing of inputs and weights using sigmoid activation function\n",
    "        self.layer1 = relu(np.dot(self.weights1, self.inputs) + self.b1)\n",
    "        self.layer2 = sigmoid(np.dot(self.weights2, self.layer1) + self.b2)\n",
    "        self.output = softmax(np.dot(self.weights3, self.layer2) + self.b3)\n",
    "\n",
    "    def backprop(self):\n",
    "        # backward processing of appling the chain rule to find derivative of the loss function with respect to weights\n",
    "        delta3 = (self.output - self.target)  / inputdata.shape[0] # average of total error is considered\n",
    "        delta2 = np.dot(self.weights3.T, delta3) * sigmoid_deriv(self.layer2)\n",
    "        delta1 = np.dot(self.weights2.T, delta2) * relu_deriv(self.layer1)\n",
    "        dw3 = np.dot(delta3, self.layer2.T)\n",
    "        dw2 = np.dot(delta2, self.layer1.T)\n",
    "        dw1 = np.dot(delta1, self.inputs.T)\n",
    "\n",
    "        # update the weights with the derivative of the loss function   \n",
    "        self.weights3 -= eta * dw3 / batchsize\n",
    "        self.weights2 -= eta * dw2 / batchsize\n",
    "        self.weights1 -= eta * dw1 / batchsize\n",
    "\n",
    "        # update biases with the derivative of the loss function\n",
    "        self.b3 -= eta * np.sum(delta3, axis = 1, keepdims=True) / batchsize # add in row axis, keeping column dimension\n",
    "        self.b2 -= eta * np.sum(delta2, axis = 1, keepdims=True) / batchsize # add in row axis, keeping column dimension\n",
    "        self.b1 -= eta * np.sum(delta1, axis = 1, keepdims=True) / batchsize # add in row axis, keeping column dimension\n",
    "\n",
    "    def predict(self, x):\n",
    "        # predict the output for a given input x\n",
    "        self.layer1 = sigmoid(np.dot(self.weights1, x) + self.b1)\n",
    "        self.layer2 = sigmoid(np.dot(self.weights2, self.layer1) + self.b2)\n",
    "        self.output = softmax(np.dot(self.weights3, self.layer2) + self.b3)\n",
    "        return (self.output)\n",
    "        \n",
    "     # calculate error\n",
    "    def calculate_error(self):\n",
    "        error = np.sum(-self.target * np.log(self.output)) # cross entropy loss function\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "+ The same data set as in layer-2 NN is used.\n",
    "+ training_loss is defined here to store the error at each iteration of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # get training data set\n",
    "    samples = 10\n",
    "    attributes = 2\n",
    "    classes = 3\n",
    "    \n",
    "    np.random.seed(45)\n",
    "    data1_set = np.random.randn(samples, attributes) + np.array([0, -3])  # red ball\n",
    "    data2_set = np.random.randn(samples, attributes) + np.array([3,  3])  # green ball\n",
    "    data3_set = np.random.randn(samples, attributes) + np.array([-3, 3])  # blue ball\n",
    "   \n",
    "    feature_set = np.vstack([data1_set, data2_set, data3_set])   \n",
    "    labels = np.array([0]*samples + [1]*samples + [2]*samples)\n",
    "\n",
    "    # Initializing colors and building a colormap\n",
    "    cmap = mpl.colors.ListedColormap(['red', 'green', 'blue'])\n",
    "\n",
    "    # display data training data set\n",
    "    # plt.figure(figsize=(10,7)) \n",
    "#    plt.scatter(feature_set[:,0], feature_set[:,1], c=labels, cmap =cmap, s= 200, alpha=0.5)  \n",
    "#    plt.show()\n",
    "\n",
    "    one_hot_labels = np.zeros((samples*classes, classes))\n",
    "\n",
    "    for i in range(samples*classes):  \n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "\n",
    "    inputdata = feature_set\n",
    "    targetvalue = one_hot_labels\n",
    "    batchsize = inputdata.shape[0]\n",
    "\n",
    "    '''\n",
    "    w1 = np.random.rand(4, inputdata.shape[1]) / np.sqrt(inputdata.shape[1]/2) # number of nueron nodes at a layer 1 layer is 4\n",
    "    w2 = np.random.rand(5, 4) / np.sqrt(4/2)   # number of nueron nodes at a layer 1 layer is 5\n",
    "    w3 = np.random.rand(3, 5) /  np.sqrt(5/2)  # number of output labels is 3\n",
    "    '''\n",
    "\n",
    "    w1 = np.random.rand(4, inputdata.shape[1]) # number of nueron nodes at a layer 1 layer is 4\n",
    "    w2 = np.random.rand(5, 4)                  # number of nueron nodes at a layer 1 layer is 5\n",
    "    w3 = np.random.rand(3, 5)                  # number of output labels is 3\n",
    "\n",
    "    nn = neuralnetwork(inputdata, w1, w2, w3, targetvalue)\n",
    "\n",
    "    training_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent Optimization\n",
    "+ All the input data are processed in batch at both forward and backward propagations.\n",
    "+ Batch GD optimizes more smoothly since the weights update are performed in batch\n",
    "+ Loss graph can be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "output after training\n",
      "[[9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [9.97605010e-01 1.13414072e-03 1.26084961e-03]\n",
      " [4.77106228e-04 9.98869768e-01 6.53125834e-04]\n",
      " [3.91783233e-04 9.98967640e-01 6.40577243e-04]\n",
      " [4.11701137e-04 9.98875411e-01 7.12888044e-04]\n",
      " [3.88137357e-04 9.98978212e-01 6.33650539e-04]\n",
      " [3.93805911e-04 9.98987423e-01 6.18771441e-04]\n",
      " [3.94292805e-04 9.98964353e-01 6.41354173e-04]\n",
      " [3.92659339e-04 9.98960881e-01 6.46459476e-04]\n",
      " [3.84768419e-04 9.98988388e-01 6.26843585e-04]\n",
      " [3.90378744e-04 9.98972000e-01 6.37621563e-04]\n",
      " [3.95143431e-04 9.98963611e-01 6.41245072e-04]\n",
      " [8.80644689e-04 7.93398614e-04 9.98325957e-01]\n",
      " [8.88890782e-04 8.14914759e-04 9.98296194e-01]\n",
      " [8.82093131e-04 7.98246062e-04 9.98319661e-01]\n",
      " [8.80718326e-04 7.93488653e-04 9.98325793e-01]\n",
      " [1.05225348e-03 9.76181644e-04 9.97971565e-01]\n",
      " [8.78307266e-04 7.85863447e-04 9.98335829e-01]\n",
      " [8.93189262e-04 8.23070046e-04 9.98283741e-01]\n",
      " [8.84911071e-04 8.06034085e-04 9.98309055e-01]\n",
      " [1.10176083e-03 1.00946965e-03 9.97888770e-01]\n",
      " [8.82836470e-04 8.00492061e-04 9.98316671e-01]]\n",
      "Predicted data based on trained weights: \n",
      "Input:  [[ 1. -1.]]\n",
      "Output:  [[9.82098641e-01 1.74080507e-02 4.93308596e-04]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training \n",
    "    for i in range(epoch):    \n",
    "        nn.forwardprop()\n",
    "        nn.backprop()\n",
    "        if (i % 1000) == 0:\n",
    "#            print(\"Error: \", nn.calculate_error())\n",
    "            training_loss.append(nn.calculate_error())\n",
    "            \n",
    "    print(\"target\")   \n",
    "    print(targetvalue)\n",
    "    print(\"output after training\")   \n",
    "    print(nn.output.T)\n",
    "\n",
    "    # predicting and testing the output for a given input data\n",
    "    # For a given input data [1, -1], it is predicated as red ball since the output is [0, 1, 0], i.e. the red ball.\n",
    "    x_prediction = np.array([[1.0, -1.0]])\n",
    "    predicted_output = nn.predict(x_prediction.T)\n",
    "    print(\"Predicted data based on trained weights: \")\n",
    "    print(\"Input: \", x_prediction)\n",
    "    print(\"Output: \", predicted_output.T)\n",
    "\n",
    "#    print(training_loss)\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_loss) + 1)\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.plot(epoch_count, training_loss, 'b-')\n",
    "    plt.legend(['Training Loss'])\n",
    "    plt.xlabel('Epoch(x1000)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
