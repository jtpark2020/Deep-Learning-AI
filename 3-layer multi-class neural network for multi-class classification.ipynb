{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 3-layer multi-class neural network for multi-class classification\n",
    "+ Network architecture: number of neurons at layer 1, layer 2, output = (4, 5, 3)\n",
    "+ Activation functions employed at layer 1, layer 2 and output are ReLu, sigmoid, and softmax, respectively.\n",
    "+ In addition to adding another layer, the ReLu activation function is adopted at layer 1 instead of sigmoid funtion.\n",
    "+ ReLu tends to prevent the weights not to be drastically reduced as the number of layers grows. \n",
    "+ By expermentation, it can be shown that the optimization depends on the learning rate.\n",
    "+ For example, the learning rate of 0.7 does not lead to the optimization. This leads to the development of more efficient\n",
    "+ optimization technique such as moment, ADAM, RMSprop, etc. These techniques try to adjust the learning rate in accordance with\n",
    "+ specific property of features of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "eta = 0.9 # learning rate\n",
    "epoch = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu Activation Function\n",
    "+ ReLu activation function and its derivative are defined.\n",
    "+ ReLU stands for rectified linear unit, and is a type of activation function. Mathematically, it is defined as \n",
    "+ y = max(0, + x). Visually, it looks like the following: ReLU is the most commonly used activation function in neural \n",
    "+ networks, especially in CNNs. The rectified linear activation function overcomes the vanishing gradient problem, \n",
    "+ allowing models to learn faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    # softmax   \n",
    "    e = np.exp(x - np.max(x))    # x-m is used to avoid overflow of exponent function\n",
    "    return e / np.sum(e, axis=0) # add in column\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-layer Neural Network Model with Softmax Function\n",
    "+ Softmax function is used at outputs of layer 3.\n",
    "+ ReLu is used for layer 1 activation function, sigmoid for layer 2, and finally softmax for layer 3, i.e., output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnetwork:\n",
    "    # neural network model\n",
    "    \n",
    "    def __init__(self, x, w1, w2, w3, y):\n",
    "        self.inputs   = x.T\n",
    "        self.target  = y.T        # target\n",
    "        self.weights1 = w1        # weights at layer 1\n",
    "        self.weights2 = w2        # weights at layer 2\n",
    "        self.weights3 = w3        # weights at output layer\n",
    "        self.b1 = np.zeros((4,1)) # bias at layer 1 \n",
    "        self.b2 = np.zeros((5,1)) # bias at layer 2 \n",
    "        self.b3 = np.zeros((3,1)) # bias at output layer   \n",
    "        self.layer1  = np.zeros((4, self.inputs.shape[1])) # output at layer 1\n",
    "        self.layer2  = np.zeros((5, self.inputs.shape[1])) # output at layer 2\n",
    "        self.output   = np.zeros(self.target.shape) # output of output layer\n",
    "    \n",
    "    def forwardprop(self):\n",
    "        # forward processing of inputs and weights using sigmoid activation function\n",
    "        self.layer1 = relu(np.dot(self.weights1, self.inputs) + self.b1)\n",
    "        self.layer2 = sigmoid(np.dot(self.weights2, self.layer1) + self.b2)\n",
    "        self.output = softmax(np.dot(self.weights3, self.layer2) + self.b3)\n",
    "\n",
    "    def backprop(self):\n",
    "        # backward processing of appling the chain rule to find derivative of the loss function with respect to weights\n",
    "        delta3 = (self.output - self.target) \n",
    "        delta2 = np.dot(self.weights3.T, delta3) * sigmoid_deriv(self.layer2)\n",
    "        delta1 = np.dot(self.weights2.T, delta2) * relu_deriv(self.layer1)\n",
    "        dw3 = np.dot(delta3, self.layer2.T)\n",
    "        dw2 = np.dot(delta2, self.layer1.T)\n",
    "        dw1 = np.dot(delta1, self.inputs.T)\n",
    "\n",
    "        # update the weights with the derivative of the loss function   \n",
    "        self.weights3 -= eta * dw3 / batchsize\n",
    "        self.weights2 -= eta * dw2 / batchsize\n",
    "        self.weights1 -= eta * dw1 / batchsize\n",
    "\n",
    "        # update biases with the derivative of the loss function\n",
    "        self.b3 -= eta * np.sum(delta3, axis = 1, keepdims=True) / batchsize # add in row axis, keeping column dimension\n",
    "        self.b2 -= eta * np.sum(delta2, axis = 1, keepdims=True) / batchsize # add in row axis, keeping column dimension\n",
    "        self.b1 -= eta * np.sum(delta1, axis = 1, keepdims=True) / batchsize # add in row axis, keeping column dimension\n",
    "\n",
    "    def predict(self, x):\n",
    "        # predict the output for a given input x\n",
    "        self.layer1 = sigmoid(np.dot(self.weights1, x) + self.b1)\n",
    "        self.layer2 = sigmoid(np.dot(self.weights2, self.layer1) + self.b2)\n",
    "        self.output = softmax(np.dot(self.weights3, self.layer2) + self.b3)\n",
    "        return (self.output)\n",
    "        \n",
    "     # calculate error\n",
    "    def calculate_error(self):\n",
    "        error = np.sum(-self.target * np.log(self.output)) # cross entropy loss function\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "+ The same data set as in layer-2 NN is used.\n",
    "+ training_loss is defined here to store the error at each iteration of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # get training data set\n",
    "    samples = 10\n",
    "    attributes = 2\n",
    "    classes = 3\n",
    "    \n",
    "    np.random.seed(45)\n",
    "    data1_set = np.random.randn(samples, attributes) + np.array([0, -3])  # red ball\n",
    "    data2_set = np.random.randn(samples, attributes) + np.array([3,  3])  # green ball\n",
    "    data3_set = np.random.randn(samples, attributes) + np.array([-3, 3])  # blue ball\n",
    "   \n",
    "    feature_set = np.vstack([data1_set, data2_set, data3_set])   \n",
    "    labels = np.array([0]*samples + [1]*samples + [2]*samples)\n",
    "\n",
    "    # Initializing colors and building a colormap\n",
    "    cmap = mpl.colors.ListedColormap(['red', 'green', 'blue'])\n",
    "\n",
    "    # display data training data set\n",
    "    # plt.figure(figsize=(10,7)) \n",
    "#    plt.scatter(feature_set[:,0], feature_set[:,1], c=labels, cmap =cmap, s= 200, alpha=0.5)  \n",
    "#    plt.show()\n",
    "\n",
    "    one_hot_labels = np.zeros((samples*classes, classes))\n",
    "\n",
    "    for i in range(samples*classes):  \n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "\n",
    "    inputdata = feature_set\n",
    "    targetvalue = one_hot_labels\n",
    "    batchsize = inputdata.shape[0]\n",
    "\n",
    "    '''\n",
    "    w1 = np.random.rand(4, inputdata.shape[1]) / np.sqrt(inputdata.shape[1]/2) # number of nueron nodes at a layer 1 layer is 4\n",
    "    w2 = np.random.rand(5, 4) / np.sqrt(4/2)   # number of nueron nodes at a layer 1 layer is 5\n",
    "    w3 = np.random.rand(3, 5) /  np.sqrt(5/2)  # number of output labels is 3\n",
    "    '''\n",
    "\n",
    "    w1 = np.random.rand(4, inputdata.shape[1]) # number of nueron nodes at a layer 1 layer is 4\n",
    "    w2 = np.random.rand(5, 4)                  # number of nueron nodes at a layer 1 layer is 5\n",
    "    w3 = np.random.rand(3, 5)                  # number of output labels is 3\n",
    "\n",
    "    nn = neuralnetwork(inputdata, w1, w2, w3, targetvalue)\n",
    "\n",
    "    training_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent Optimization\n",
    "+ All the input data are processed in batch at both forward and backward propagations.\n",
    "+ Batch GD optimizes more smoothly since the weights update are performed in batch\n",
    "+ Loss graph can be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "output after training\n",
      "[[9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [9.99853898e-01 1.00325723e-04 4.57760500e-05]\n",
      " [4.73067716e-05 9.99910997e-01 4.16962677e-05]\n",
      " [3.77279693e-05 9.99919221e-01 4.30514334e-05]\n",
      " [3.87824792e-05 9.99915155e-01 4.60622488e-05]\n",
      " [3.75678381e-05 9.99919602e-01 4.28303951e-05]\n",
      " [3.82048218e-05 9.99919531e-01 4.22641372e-05]\n",
      " [3.78787311e-05 9.99919066e-01 4.30548452e-05]\n",
      " [3.77721852e-05 9.99918963e-01 4.32644397e-05]\n",
      " [3.74416263e-05 9.99919917e-01 4.26409702e-05]\n",
      " [3.76641405e-05 9.99919382e-01 4.29535300e-05]\n",
      " [3.79494549e-05 9.99919015e-01 4.30358774e-05]\n",
      " [3.63219231e-05 5.82696480e-05 9.99905408e-01]\n",
      " [3.64603993e-05 5.87774284e-05 9.99904762e-01]\n",
      " [3.63365234e-05 5.83134313e-05 9.99905350e-01]\n",
      " [3.63081045e-05 5.81677560e-05 9.99905524e-01]\n",
      " [4.15806612e-05 6.63855669e-05 9.99892034e-01]\n",
      " [3.62807594e-05 5.81236208e-05 9.99905596e-01]\n",
      " [3.65528277e-05 5.90512316e-05 9.99904396e-01]\n",
      " [3.64024970e-05 5.85813181e-05 9.99905016e-01]\n",
      " [4.29600243e-05 6.78611797e-05 9.99889179e-01]\n",
      " [3.63571693e-05 5.84048319e-05 9.99905238e-01]]\n",
      "Predicted data based on trained weights: \n",
      "Input:  [[ 1. -1.]]\n",
      "Output:  [[9.98432290e-01 1.56043210e-03 7.27839208e-06]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training \n",
    "    for i in range(epoch):    \n",
    "        nn.forwardprop()\n",
    "        nn.backprop()\n",
    "        if (i % 1000) == 0:\n",
    "#            print(\"Error: \", nn.calculate_error())\n",
    "            training_loss.append(nn.calculate_error())\n",
    "            \n",
    "    print(\"target\")   \n",
    "    print(targetvalue)\n",
    "    print(\"output after training\")   \n",
    "    print(nn.output.T)\n",
    "\n",
    "    # predicting and testing the output for a given input data\n",
    "    # For a given input data [1, -1], it is predicated as red ball since the output is [0, 1, 0], i.e. the red ball.\n",
    "    x_prediction = np.array([[1.0, -1.0]])\n",
    "    predicted_output = nn.predict(x_prediction.T)\n",
    "    print(\"Predicted data based on trained weights: \")\n",
    "    print(\"Input: \", x_prediction)\n",
    "    print(\"Output: \", predicted_output.T)\n",
    "\n",
    "#    print(training_loss)\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_loss) + 1)\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.plot(epoch_count, training_loss, 'b-')\n",
    "    plt.legend(['Training Loss'])\n",
    "    plt.xlabel('Epoch(x1000)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
