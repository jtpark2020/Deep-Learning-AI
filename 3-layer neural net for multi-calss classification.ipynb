{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 3-layer multi-class neural network with softmax function with bias for multi-class classification.\n",
    "+ Network architecture: number of neurons at layer 1, layer 2, output = (4, 5, 3)\n",
    "+ Activation functions employed at layer 1, layer 2 and output are ReLu, sigmoid, and softmax, respectively.\n",
    "+ In addition to adding another layer, the ReLu activation function is adopted at layer 1 instead of sigmoid funtion.\n",
    "+ ReLu tends to prevent the weights not to be drastically reduced as the number of layers grows. \n",
    "+ By expermentation, it can be shown that the optimization depends on the learning rate.\n",
    "+ For example, the learning rate of 0.7 does not lead to the optimization. This leads to the development of more efficient\n",
    "+ optimization technique such as moment, ADAM, RMSprop, etc. These techniques try to adjust the learning rate in accordance with\n",
    "+ specific property of features of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "eta = 0.05  # learning rate\n",
    "epoch = 15000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu Activation Function\n",
    "+ ReLu activation function and its derivative are defined.\n",
    "+ ReLU stands for rectified linear unit, and is a type of activation function. Mathematically, it is defined as \n",
    "+ y = max(0, + x). Visually, it looks like the following: ReLU is the most commonly used activation function in neural \n",
    "+ networks, especially in CNNs. The rectified linear activation function overcomes the vanishing gradient problem, \n",
    "+ allowing models to learn faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    # softmax   \n",
    "    e = np.exp(x - np.max(x))    # x-m is used to avoid overflow of exponent function\n",
    "    return e / np.sum(e, axis=0) # add in column\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-layer Neural Network Model with Softmax Function\n",
    "+ Softmax function is used at outputs of layer 3.\n",
    "+ ReLu is used for layer 1 activation function, sigmoid for layer 2, and finally softmax for layer 3, i.e., output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnetwork:\n",
    "    # neural network model\n",
    "    \n",
    "    def __init__(self, x, w1, w2, w3, y):\n",
    "        self.inputs   = x.T\n",
    "        self.target  = y.T        # target\n",
    "        self.weights1 = w1        # weights at layer 1\n",
    "        self.weights2 = w2        # weights at layer 2\n",
    "        self.weights3 = w3        # weights at output layer\n",
    "        self.b1 = np.zeros((4,1)) # bias at layer 1 \n",
    "        self.b2 = np.zeros((5,1)) # bias at layer 2 \n",
    "        self.b3 = np.zeros((3,1)) # bias at output layer   \n",
    "        self.layer1  = np.zeros((4, self.inputs.shape[1])) # output at layer 1\n",
    "        self.layer2  = np.zeros((5, self.inputs.shape[1])) # output at layer 2\n",
    "        self.output   = np.zeros(self.target.shape) # output of output layer\n",
    "    \n",
    "    def forwardprop(self):\n",
    "        # forward processing of inputs and weights using sigmoid activation function\n",
    "        self.layer1 = relu(np.dot(self.weights1, self.inputs) + self.b1)\n",
    "        self.layer2 = sigmoid(np.dot(self.weights2, self.layer1) + self.b2)\n",
    "        self.output = softmax(np.dot(self.weights3, self.layer2) + self.b3)\n",
    "\n",
    "    def backprop(self):\n",
    "        # backward processing of appling the chain rule to find derivative of the loss function with respect to weights\n",
    "        delta3 = (self.output - self.target)  / inputdata.shape[0] # average of total error is considered\n",
    "        delta2 = np.dot(self.weights3.T, delta3) * sigmoid_deriv(self.layer2)\n",
    "        delta1 = np.dot(self.weights2.T, delta2) * relu_deriv(self.layer1)\n",
    "        dw3 = np.dot(delta3, self.layer2.T)\n",
    "        dw2 = np.dot(delta2, self.layer1.T)\n",
    "        dw1 = np.dot(delta1, self.inputs.T)\n",
    "\n",
    "        # update the weights with the derivative of the loss function   \n",
    "        self.weights3 -= eta * dw3\n",
    "        self.weights2 -= eta * dw2\n",
    "        self.weights1 -= eta * dw1\n",
    "\n",
    "        # update biases with the derivative of the loss function\n",
    "        self.b3 -= eta * np.sum(delta3, axis = 1, keepdims=True) # add in row axis, keeping column dimension\n",
    "        self.b2 -= eta * np.sum(delta2, axis = 1, keepdims=True) # add in row axis, keeping column dimension\n",
    "        self.b1 -= eta * np.sum(delta1, axis = 1, keepdims=True) # add in row axis, keeping column dimension\n",
    "\n",
    "    def predict(self, x):\n",
    "        # predict the output for a given input x\n",
    "        self.layer1 = sigmoid(np.dot(self.weights1, x) + self.b1)\n",
    "        self.layer2 = sigmoid(np.dot(self.weights2, self.layer1) + self.b2)\n",
    "        self.output = softmax(np.dot(self.weights3, self.layer2) + self.b3)\n",
    "        return (self.output)\n",
    "        \n",
    "     # calculate error\n",
    "    def calculate_error(self):\n",
    "        error = np.sum(-self.target * np.log(self.output)) # cross entropy loss function\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "+ The same data set as in layer-2 NN is used.\n",
    "+ training_loss is defined here to store the error at each iteration of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # get training data set\n",
    "    samples = 10\n",
    "    attributes = 2\n",
    "    classes = 3\n",
    "    \n",
    "    np.random.seed(45)\n",
    "    data1_set = np.random.randn(samples, attributes) + np.array([0, -3])  # red ball\n",
    "    data2_set = np.random.randn(samples, attributes) + np.array([3,  3])  # green ball\n",
    "    data3_set = np.random.randn(samples, attributes) + np.array([-3, 3])  # blue ball\n",
    "   \n",
    "    feature_set = np.vstack([data1_set, data2_set, data3_set])   \n",
    "    labels = np.array([0]*samples + [1]*samples + [2]*samples)\n",
    "\n",
    "    # Initializing colors and building a colormap\n",
    "    cmap = mpl.colors.ListedColormap(['red', 'green', 'blue'])\n",
    "\n",
    "    # display data training data set\n",
    "    # plt.figure(figsize=(10,7)) \n",
    "#    plt.scatter(feature_set[:,0], feature_set[:,1], c=labels, cmap =cmap, s= 200, alpha=0.5)  \n",
    "#    plt.show()\n",
    "\n",
    "    one_hot_labels = np.zeros((samples*classes, classes))\n",
    "\n",
    "    for i in range(samples*classes):  \n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "\n",
    "    inputdata = feature_set\n",
    "    targetvalue = one_hot_labels\n",
    "\n",
    "    '''\n",
    "    w1 = np.random.rand(4, inputdata.shape[1]) / np.sqrt(inputdata.shape[1]/2) # number of nueron nodes at a layer 1 layer is 4\n",
    "    w2 = np.random.rand(5, 4) / np.sqrt(4/2)   # number of nueron nodes at a layer 1 layer is 5\n",
    "    w3 = np.random.rand(3, 5) /  np.sqrt(5/2)  # number of output labels is 3\n",
    "    '''\n",
    "\n",
    "    w1 = np.random.rand(4, inputdata.shape[1]) # number of nueron nodes at a layer 1 layer is 4\n",
    "    w2 = np.random.rand(5, 4)                  # number of nueron nodes at a layer 1 layer is 5\n",
    "    w3 = np.random.rand(3, 5)                  # number of output labels is 3\n",
    "\n",
    "    nn = neuralnetwork(inputdata, w1, w2, w3, targetvalue)\n",
    "\n",
    "    training_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent Optimization\n",
    "+ All the input data are processed in batch at both forward and backward propagations.\n",
    "+ Batch GD optimizes more smoothly since the weights update are performed in batch\n",
    "+ Loss graph can be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "    for i in range(epoch):    \n",
    "        nn.forwardprop()\n",
    "        nn.backprop()\n",
    "        if (i % 1000) == 0:\n",
    "#            print(\"Error: \", nn.calculate_error())\n",
    "            training_loss.append(nn.calculate_error())\n",
    "            \n",
    "    print(\"target\")   \n",
    "    print(targetvalue)\n",
    "    print(\"output after training\")   \n",
    "    print(nn.output.T)\n",
    "\n",
    "    # predicting and testing the output for a given input data\n",
    "    # For a given input data [1, -1], it is predicated as red ball since the output is [0, 1, 0], i.e. the red ball.\n",
    "    x_prediction = np.array([[1.0, -1.0]])\n",
    "    predicted_output = nn.predict(x_prediction.T)\n",
    "    print(\"Predicted data based on trained weights: \")\n",
    "    print(\"Input: \", x_prediction)\n",
    "    print(\"Output: \", predicted_output.T)\n",
    "\n",
    "#    print(training_loss)\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_loss) + 1)\n",
    "    plt.plot(epoch_count, training_loss, 'b-')\n",
    "    plt.legend(['Training Loss'])\n",
    "    plt.xlabel('Epoch(x1000)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
